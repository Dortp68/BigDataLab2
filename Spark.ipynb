{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fb6f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a286a",
   "metadata": {},
   "source": [
    "# Подключение к кластеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fd55d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 14:19:58 WARN Utils: Your hostname, dortp58-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/10/12 14:19:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/12 14:19:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Lab2\")\n",
    "    .getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05cc0b",
   "metadata": {},
   "source": [
    "# Чтение и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f3f928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': dtype('int64'),\n",
       " 'Name': dtype('O'),\n",
       " 'RatingDist1': dtype('O'),\n",
       " 'pagesNumber': dtype('int64'),\n",
       " 'RatingDist4': dtype('O'),\n",
       " 'RatingDistTotal': dtype('O'),\n",
       " 'PublishMonth': dtype('int64'),\n",
       " 'PublishDay': dtype('int64'),\n",
       " 'Publisher': dtype('O'),\n",
       " 'CountsOfReview': dtype('int64'),\n",
       " 'PublishYear': dtype('int64'),\n",
       " 'Language': dtype('O'),\n",
       " 'Authors': dtype('O'),\n",
       " 'Rating': dtype('float64'),\n",
       " 'RatingDist2': dtype('O'),\n",
       " 'RatingDist5': dtype('O'),\n",
       " 'ISBN': dtype('O'),\n",
       " 'RatingDist3': dtype('O')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"book1-100k.csv\")\n",
    "schema = df.dtypes.to_dict()\n",
    "schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa946ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"PagesNumber\", IntegerType()),\n",
    "    StructField(\"Authors\", StringType()),\n",
    "    StructField(\"Publisher\", StringType()),\n",
    "    StructField(\"Language\", StringType()),\n",
    "    StructField(\"Rating\", DoubleType()),\n",
    "    StructField(\"RatingDist1\", StringType()),\n",
    "    StructField(\"RatingDist2\", StringType()),\n",
    "    StructField(\"RatingDist3\", StringType()),\n",
    "    StructField(\"RatingDist4\", StringType()),\n",
    "    StructField(\"RatingDist5\", StringType()),\n",
    "    StructField(\"RatingDistTotal\", StringType()),\n",
    "    StructField(\"CountsOfReview\", IntegerType()),\n",
    "    StructField(\"PublishDay\", IntegerType()),\n",
    "    StructField(\"PublishMonth\", IntegerType()),\n",
    "    StructField(\"PublishYear\", IntegerType()),\n",
    "    StructField(\"ISBN\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad32159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [i.name for i in book_schema.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9612a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./book400k-500k.csv\n",
      "./book900k-1000k.csv\n",
      "./book1400k-1500k.csv\n",
      "./book600k-700k.csv\n",
      "./book1200k-1300k.csv\n",
      "./book1700k-1800k.csv\n",
      "./book1900k-2000k.csv\n",
      "./book700k-800k.csv\n",
      "./book3000k-4000k.csv\n",
      "./book1500k-1600k.csv\n",
      "./book4000k-5000k.csv\n",
      "./book1000k-1100k.csv\n",
      "./book1800k-1900k.csv\n",
      "./book1600k-1700k.csv\n",
      "./book1300k-1400k.csv\n",
      "./book500k-600k.csv\n",
      "./book800k-900k.csv\n",
      "./book300k-400k.csv\n",
      "./book200k-300k.csv\n",
      "./book1100k-1200k.csv\n",
      "./book100k-200k.csv\n",
      "./book2000k-3000k.csv\n",
      "./book1-100k.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "all_files = glob.glob(r'./' + \"/b*.csv\")\n",
    "books = spark.createDataFrame([], schema=book_schema)\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    if 'PagesNumber' not in df.columns:\n",
    "        df = df.rename(columns={\"pagesNumber\": \"PagesNumber\"})\n",
    "    spak_df = spark.createDataFrame(df[cols], schema=book_schema)\n",
    "    books = books.union(spak_df)\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc0061",
   "metadata": {},
   "source": [
    "Читаем файлы user_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5aae9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': dtype('int64'), 'Name': dtype('O'), 'Rating': dtype('O')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"user_rating_0_to_1000.csv\")\n",
    "schema = df.dtypes.to_dict()\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ea58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Rating\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6914f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_u = [\"ID\", \"Name\", \"Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1d82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./user_rating_4000_to_5000.csv\n",
      "./user_rating_2000_to_3000.csv\n",
      "./user_rating_3000_to_4000.csv\n",
      "./user_rating_6000_to_11000.csv\n",
      "./user_rating_1000_to_2000.csv\n",
      "./user_rating_0_to_1000.csv\n",
      "./user_rating_5000_to_6000.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "all_files = glob.glob(r'./' + \"/u*.csv\")\n",
    "users = spark.createDataFrame([], schema=user_schema)\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    spak_df = spark.createDataFrame(df[cols_u], schema=user_schema)\n",
    "    users = users.union(spak_df)\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355da02d",
   "metadata": {},
   "source": [
    "Преобразование в parquet и запись"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fae51",
   "metadata": {},
   "source": [
    "На hdfs никак не получается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "50eeaa6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o813.parquet.\n: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9870/user/books.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/myenv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/myenv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/myenv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/myenv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o813.parquet.\n: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n"
     ]
    }
   ],
   "source": [
    "books.write.parquet(\"hdfs://localhost:9870/user/books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8c5cbd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 00:33:35 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:33:53 WARN TaskSetManager: Stage 9 contains a task of very large size (2954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/10/12 00:33:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:33:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/10/12 00:33:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:33:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/10/12 00:33:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/10/12 00:34:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:05 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:40 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:41 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:41 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/10/12 00:34:42 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:43 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/10/12 00:34:43 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/10/12 00:34:44 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/10/12 00:34:45 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/10/12 00:34:46 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/10/12 00:34:47 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/10/12 00:34:47 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/10/12 00:34:47 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:34:48 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/10/12 00:35:31 WARN TaskSetManager: Stage 11 contains a task of very large size (2954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "books.write.parquet(\"books.parquet\",mode = \"overwrite\")\n",
    "users.write.parquet(\"users.parquet\",mode = \"overwrite\")\n",
    "books.write.csv(\"books.csv\",mode = \"overwrite\")\n",
    "users.write.csv(\"users.csv\",mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7307a8",
   "metadata": {},
   "source": [
    "# Измерение скорости чтения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0dc82b",
   "metadata": {},
   "source": [
    "Для датасета книг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3a2feeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время чтения parquet:  0.289625883102417\n",
      "Время чтения сsv:  2.851456880569458\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "books_parquet = spark.read.parquet(\"books.parquet\")\n",
    "end_p = time.time()-start\n",
    "\n",
    "start = time.time()\n",
    "books_csv = spark.read.csv(\"books.csv\")\n",
    "end_csv = time.time()-start\n",
    "\n",
    "print(\"Время чтения parquet: \", end_p)\n",
    "print(\"Время чтения сsv: \", end_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7404ad",
   "metadata": {},
   "source": [
    "Для датасета юзер скоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "13d83c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время чтения parquet:  0.18186259269714355\n",
      "Время чтения сsv:  0.39438915252685547\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "users_parquet = spark.read.parquet(\"users.parquet\")\n",
    "end_p = time.time()-start\n",
    "\n",
    "start = time.time()\n",
    "users_csv = spark.read.csv(\"users.csv\")\n",
    "end_csv = time.time()-start\n",
    "\n",
    "print(\"Время чтения parquet: \", end_p)\n",
    "print(\"Время чтения сsv: \", end_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de818c1",
   "metadata": {},
   "source": [
    "Топ 10 книг с наибольшим числом ревью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07da475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 14:36:58 WARN TaskSetManager: Stage 5 contains a task of very large size (2954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 5:======================================================>(287 + 1) / 288]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Name|\n",
      "+--------------------+\n",
      "|The Hunger Games ...|\n",
      "|Twilight (Twiligh...|\n",
      "|      The Book Thief|\n",
      "|            The Help|\n",
      "|Harry Potter and ...|\n",
      "|The Giver (The Gi...|\n",
      "| Water for Elephants|\n",
      "|The Girl with the...|\n",
      "|Harry Potter and ...|\n",
      "|The Lightning Thi...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "books.sort(books.CountsOfReview.desc()).select(\"Name\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150e10e",
   "metadata": {},
   "source": [
    "Топ 10 издателей с наибольшим средним числом страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ddbd012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 14:52:59 WARN TaskSetManager: Stage 9 contains a task of very large size (2954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           Publisher|  avg(PagesNumber)|\n",
      "+--------------------+------------------+\n",
      "|Crafty Secrets Pu...|         1807321.6|\n",
      "|    Sacred-texts.com|          500000.0|\n",
      "|Department of Rus...| 322128.5714285714|\n",
      "|Logos Research Sy...|          100000.0|\n",
      "|Encyclopedia Brit...|           32642.0|\n",
      "|Progressive Manag...|        19106.3625|\n",
      "|Still Waters Revi...|10080.142857142857|\n",
      "|P. Shalom Publica...|            8539.0|\n",
      "|Hendrickson Publi...|            6448.0|\n",
      "|            IEEE/EMB|            6000.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "books.groupBy(\"Publisher\").avg(\"PagesNumber\").sort(avg(\"PagesNumber\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765a3ed",
   "metadata": {},
   "source": [
    "10 наиболее активных по число изданных книг лет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0954160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 14:58:44 WARN TaskSetManager: Stage 12 contains a task of very large size (2954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 12:====================================================> (282 + 6) / 288]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|PublishYear| count|\n",
      "+-----------+------+\n",
      "|       2007|129507|\n",
      "|       2006|122374|\n",
      "|       2005|117639|\n",
      "|       2004|105733|\n",
      "|       2003|104345|\n",
      "|       2002| 95537|\n",
      "|       2001| 88228|\n",
      "|       2000| 87290|\n",
      "|       2008| 80265|\n",
      "|       1999| 80155|\n",
      "+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "books.groupBy(\"PublishYear\").count().sort(count('*').desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0c959",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04a0e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "rating_map = {\n",
    "    \"it was amazing\" : 5,\n",
    "    \"really liked it\" : 4,\n",
    "    \"liked it\" : 3,\n",
    "    \"it was ok\" : 2,\n",
    "    \"did not like it\" : 1\n",
    "}\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*rating_map.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cc0566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'map(it was amazing, 5, really liked it, 4, liked it, 3, it was ok, 2, did not like it, 1)'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a06d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = spark.readStream\\\n",
    "    .schema(user_schema)\\\n",
    "    .parquet(\"./users.parquet\")\\\n",
    "    .filter(col(\"Rating\") != \"This user doesn't have any rating\")\\\n",
    "    .withColumn(\"RatingValue\", mapping_expr[col('Rating')])\\\n",
    "    .withColumn(\"timestamp\", current_timestamp())\\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\"Name\", window(col(\"timestamp\"), \"15 seconds\"))\\\n",
    "    .agg(avg(\"RatingValue\").alias(\"AverageRating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 17:00:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 35:===========>                                          (44 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "stream_df.writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"path\", \"./book_ratings.parquet\")\\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\")\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
